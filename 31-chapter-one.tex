\chapter{Первая глава. Описание алгоритмов}
\label{cha:ch_1}
\section{Игры в развернутой форме и равновесие}
\par
Игра в развернутой форме представляют компактную общую модель взаимодействий между агентами и явно отражает последовательный характер этих взаимодействий. Последовательность принятия решений игроками в такой постановке представлена деревом решения. При этом, листья дерева отождествлены с терминальными состояниями, в которых игра завершается и игроки получают выплаты. Любой нетерминальный узел дерева представляет точку принятия решения. Неполнота информации выражается в том, что различные узлы игрового дерева считаются неразличимыми для игрока. Совокупность всех попарно неразличимых состояний игры называется информационными состояниями. Приведем формальное определение.
\par
\begin{defin}\label{def1}
Конечная игра в развернутой форме с неполной информацией содержит следующие компоненты:
\end{defin}
\begin{itemize}
	\item конечное множество игроков $N$;
	\item конечное множество историй действий игроков $H$, такое, что $\emptyset \in H$ и любой префикс элемента из $H$ также принадлежит $H$. $Z \subseteq H$ представляет множество терминальных историй (множество историй игры на являющихся префиксом). $A(h)=\{a \colon (h,\;a)\in H \}$ "--- доступные после нетерминальной истории $h\in H$ действия;
	\item функция $P\colon H\setminus Z \to N \cup \{c\}$, которая сопоставляет каждой нетерминальной истории $h\in H\setminus Z$ игрока, которому предстоит принять решение, либо игрока $c$ представляющего случайное событие;
	\item функция $f_c$, которая сопоставляет всем $h \in H$, для которых $P(h)=c$, вероятностное распределение $f_c(\cdot |h)$ на $A(h)$. $f_c(a|h)$ представляет вероятность выбора $a$ после истории $h$;
	\item для каждого игрока $i \in N$ $\mathcal{I}_i $ обозначает разбиение $ \{h \in H \colon P(h) = i\}$, для которого $A(h)=A(h')$ всякий раз когда $h$ и $h'$ принадлежат одному элементу разбиения. Для $I_i \in \mathcal{I}_i$ определим $A(I_i)=A(h)$ и $P(I_i)=i$ для всех $h \in I_i$. $\mathcal{I}_i$ называют информационным набором игрока $i$, а $I_i \in \mathcal{I}_i$ информационным состоянием игрока $i$;
	\item для каждого игрока $i \in N$ определена функция выигрыша $u_i \colon Z \to \mathbb{R}$. Если для игры в развернутой форме выполняется $\forall z \in Z \sum_{i \in N}U_i(z) = 0 $, то такую игру называют игрой с нулевой суммой. Определим $\Delta_{u,i} = max_{z\in Z}\;u_i(z) - min_{z\in Z}\;u_i(z)$ для диапазона выплат игрока. 
\end{itemize}
\par
 Отметим, что информационные наборы могут использоваться не только для реализации правил конкретной игры, но и могут быть использованы для того, чтобы заставить игрока забыть о предыдущих действиях. Игры в которых игроки не забывают о действиях называют играми с полной памятью. В дальнейшем мы будем рассматривать конечные игры в развернутой форме с полной памятью.

Стратегия игрока $i$ "--- это функция $\sigma_i$, которая ставит в соответствие каждому информационному состоянию $I_i \in \mathcal{I}_i$ вероятностное распределение на $A(I_i)$. Обозначим за $\Sigma_i$ множество всех стратегий игрока $i$. Стратегический профиль $\sigma$ содержит стратегии для каждого игрока $i \in N$. При этом за $\sigma_{-i}$ обозначим $\sigma$ без $\sigma_i$. 

Обозначим за $\pi^\sigma(h)$ вероятность того, что игроки достигнут $h$ руководствуясь $\sigma$. Мы можем представить $\pi^\sigma$ как $\pi^\sigma = \prod_{i\in N\cup\{c\}}\pi_i^\sigma(h)$, выделяя вклад каждого игрока. В таком случае, $\pi_i^\sigma(h)$ обозначает вероятность принятия совокупности решений игрока $i$, ведущих от $\emptyset$ к $h$. Иными словами
\begin{equation*} 
\pi_i^\sigma(h)=
\begin{cases}
	\prod_{h \sqsubset h' \wedge P(h')=i \wedge h \sqsubset (h',a)} \sigma(h')(a) & \{h' | h \sqsubset h' \wedge P(h')=i \} \neq \emptyset\\
	1 &\text{иначе.}
\end{cases}
\end{equation*}
Запись $h \sqsubset h'$ означает, что $h'$ является префиксом $h$. 
Обозначим за $\pi_{-i}^\sigma(h) $ вероятность достижения истории $h$ всеми игроками (включая $c$) за исключением $i$.
Для $I \subseteq H $ определим $\pi^\sigma(I) = \sum_{h\in I}\pi^\sigma(h)$. Аналогично, введем $\pi_i^\sigma(I)$ и $\pi_{-i}^\sigma(I)$. 
\par
Ожидаемое значение выплаты для игрока $i$ обозначим как $u_i(\sigma)=\sum_{h\in Z}u_i(h)\pi^\sigma(h)$. 

Традиционным способом решения игр в развернутой форме является поиск равновесного профиля стратегий $\sigma$, который удовлетворяет следующему условию 
\begin{equation}
\begin{split}
	\forall i \in N,\;\;u_i(\sigma)\geq \underset{\sigma'_i\in \Sigma_i}{\max\;} u_i(\sigma'_i,\;\sigma_{-i}).
\end{split}
\end{equation} 
Такой стратегический профиль называют равновесием по Нэшу. В случае, если стратегический профиль $\sigma$ удовлетворяет условию 
\begin{equation}
	\begin{split}
		\forall i \in N,\; \epsilon > 0, \;\;u_i(\sigma) + \epsilon \geq \underset{\sigma'_i\in \Sigma_i}{\max\;} u_i(\sigma'_i,\;\sigma_{-i}).
	\end{split}
\end{equation}
его называют $\epsilon$ – равновесием по Нэшу.
\par
Для рассматриваемых далее алгоритмов наиболее интересен вариант игры с нулевой семмой для двух игроков. Именно для него имеется строгое математическое обоснование сходимости к равновесию Нэша.

\section{Контрафактические сожаления и их минимизация}

Минимизация сожалений является популярным концептом, для построения итеративных алгоритмов приближенного решения игр в развернутой форме \cite{RegretMatching}. Приведем связанные с ней определения. Рассмотрим дискретный отрезок времени $T$ включающий $T$ раундов от $1$ до $T$. Обозначим за $\sigma_i^t$ стратегию игрока $i$ в раунде $t$. 
\begin{defin}
	Средним общим сожалением игрока $i$ на момент времени $T$ называют величину 
\end{defin}
\begin{equation}
	R_i^T=\frac{1}{T} \underset{\sigma_i^*\in \Sigma_i}{\max\;}\sum_{t=1}^{T}u_i(\sigma_i^* ,\;\sigma_{-i}^t)-u_i(\sigma^t)
\end{equation} 

В дополнении к этому, определим $\bar{\sigma}_i^T$ как среднюю стратегию относительно всех раундов от 1 до T. Таким образом для каждого $I \in \mathcal{I}_i$ и $a\in A(I)$ определим 
\begin{equation}
	\bar{\sigma}_i^T(I)=\frac{\sum^T_{t=1}\pi_i^{\sigma^t}(I)\sigma^t(I)(a)}{\sum_{t=1}^{T}\pi_i^{\sigma^t}(I)}.
\end{equation}
\begin{theo} Если для игры с двумя игроками и с нулевой суммой на момент времени $T$ средние общие сожаления игроков меньше $\epsilon$, то $\sigma$ является $2\epsilon$ равновесием \cite{NIPS07cfr}. 
\end{theo}

Говорят, что алгоритм выбора $\sigma^t$ реализует минимизацию сожалений, если средние общие сожаления игроков стремятся к нулю при $t$ стремящимся к бесконечности. И как результат, алгоритм минимизации сожалений может быть использован для нахождения приближенного равновесия по Нэшу, в случае игр двух игроков с нулевой суммой. Вообще говоря, в случае ненулевой суммы или большего числа игроков алгоритм минимизации сожалений не приводит к равновесию Нэша. Однако, он приводит к другому классу равновесий. Доказано, что полученное решение сходится к грубому коррелированному равновесию и, более того,  устраняет итеративно доминируемые действия в профилях стратегий\cite{RGibson}.

Понятие контрафактического сожаления служит для декомпозиции среднего общего сожаления в набор дополнительных сожалений, которые могут быть минимизированы независимо для каждого информационного состояния. 

Обозначим через $u_i(\sigma,\;h)$ цену игры с точки зрения истории $h$, при условии, что $h$ была достигнута, и игроки спользуют в дальнейшем $\sigma$. 

\begin{defin}
	Контрафактической ценой $u_i(\sigma,\;I)$ назовем ожидаемую цену, при условии, что информационное состояние $I$ было достигнуто, когда все игроки кроме $i$ играли в соответствии с $\sigma$. Формально 
\end{defin} 
\begin{equation}
	u_i(\sigma,\;I)=\sum_{h\in I,h'\in Z}\pi_{-i}^\sigma(h)\pi^\sigma(h,\;h')u_i(h'),
\end{equation}
где $\pi^\sigma(h,\;h')$ "--- вероятность перехода из $h$ в $h'$.

Обозначим за $\sigma^t |_{I \to a}$ стратегический профиль идентичный $\sigma$ за исключением того, что $i$ всегда выбирает $a$ в $I$. 

Средним немедленным контрафактическим сожалением назовем 

\begin{equation}\label{Sych_eq1}
	R_{i,imm}^T(I) = \frac{1}{T}\underset{a\in A(I)}{\max\;}\sum_{t=1}^{T}u_i(\sigma^t |_{I \to a},\;I)-u_i(\sigma^t,\;I).
\end{equation}

Интуитивно это выражение можно понимать как аналог среднего общего сожаления в терминах контрафактической цены. Однако, вместо рассмотрения всевозможных максимизирующих стратегий рассматриваются локальные модификации стратегии. Положим $R_{i,imm}^{T,+}(I) = \max (R_{i,imm}^{T}(I),\;0)$. Связь немедленных контрафактических сожалений и общих средних сожалений раскрывает следующая теорема. 

\begin{theo} 
	$R_i^T \leq \sum_{I\in \mathcal{I}_i}R_{i,imm}^{T,+}(I)$\cite{NIPS07cfr}.
\end{theo}

Минимизация средних немедленных контрафактических сожалений приводит к минимизации средних общих сожалений. В свою очередь, минимизация среднего немедленного контрафактического сожаления может происходить за счет минимизации выражений под функцией максимума. Таким образом, мы приходим к понятию среднего контрафактического сожаления 
\begin{equation}
	R_i^T(I,\; a) = \frac{1}{T}\sum_{t=1}^{T}u_i(\sigma^t |_{I \to a},\;I)-u_i(\sigma^t,\;I).
\end{equation}
Контрафактическое сожаление рассматривает действие в информационном состоянии. В свою очередь, для минимизации средних контрафактических сожалений можно применить алгоритм приближения Блэквела\cite{RegretMatching}, который приведет к следующей последовательности стратегий 

\begin{equation}\label{CfrTStrategy}
	\sigma_i^{T+1}(I)(a)= 
	\begin{cases}
		\frac{R_i^{T,+}(I,\;a)}{\sum_{a\in A(I)}R_i^{T,+}(I,\;a)} &\text{если $\sum_{a\in A(I)}R_i^{T,+}(I,\;a) > 0$,}\\
		\frac{1}{|A(I)|} &\text{иначе.}
	\end{cases}
\end{equation}

Другими словами, действие выбирается в пропорции соотношения позитивных контрафактических сожалений о не выборе этого действия. Обоснование сходимости полученного решения и оценку ее скорости предоставляет следующая теорема. 

\begin{theo}\label{CfrTStrategyExp}
	 Если игроки придерживаются стратегий, заданных выражением (\ref{CfrTStrategy}), то $R_{i,imm}^T(I) \leq \Delta_{u,i}\sqrt{|A_i|}/\sqrt{T}$ и, следовательно, $R_i^T \leq \Delta_{u,i}|\mathcal{I}_i|\sqrt{|A_i|}/\sqrt{T}$, где $|A_i|=\max_{h\colon P(h)=i}|A(h)|$\cite{NIPS07cfr}.
\end{theo}

\par
Таким образом, по мере увеличения числа проведенных итераций уменьшаются средние общие сожаления.